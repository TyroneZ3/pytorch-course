{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torchtext\r\n",
    "import tqdm\r\n",
    "import sys"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "C:\\Users\\Tyrone\\anaconda3\\envs\\nlp\\python.exe\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_iter = torchtext.datasets.IMDB(split='train')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "tokenizor = torchtext.data.utils.get_tokenizer('basic_english')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "tokens = []\r\n",
    "for label, line in train_iter:\r\n",
    "    tokens += tokenizor(line)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from collections import Counter, OrderedDict\r\n",
    "# from torchtext.vocab import Vocab\r\n",
    "\r\n",
    "counter = Counter(tokens) # unsorted dict\r\n",
    "sorted_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\r\n",
    "ordered_dict = OrderedDict(sorted_tuples)\r\n",
    "Vocab = torchtext.vocab.vocab(ordered_dict, min_freq=10)\r\n",
    "\r\n",
    "# adding <unk> token and default index\r\n",
    "unk_token = '<unk>'\r\n",
    "default_index = 0\r\n",
    "if unk_token not in Vocab:\r\n",
    "    Vocab.insert_token(unk_token, default_index)\r\n",
    "Vocab.set_default_index(default_index)\r\n",
    "\r\n",
    "# adding <pad> token\r\n",
    "Vocab.insert_token('<pad>', 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "from torch.utils.data import DataLoader, Dataset\r\n",
    "from torch.nn.utils.rnn import pad_sequence\r\n",
    "import random\r\n",
    "\r\n",
    "text_transform = lambda x: [Vocab[token] for token in tokenizor(x)]\r\n",
    "label_transform = lambda x: 1 if x == 'pos' else 0\r\n",
    "\r\n",
    "def collate_batch(batch):\r\n",
    "    label_list, text_list = [], []\r\n",
    "    for label, text in batch:\r\n",
    "        text_list.append(torch.tensor(text_transform(text)))\r\n",
    "        label_list.append(label_transform(label))\r\n",
    "    \r\n",
    "    return torch.tensor(label_list), pad_sequence(text_list, padding_value=Vocab['<pad>'])\r\n",
    "\r\n",
    "train_iter, test_iter = torchtext.datasets.IMDB(split=('train', 'test'))\r\n",
    "\r\n",
    "class dataset_from_iter(Dataset):\r\n",
    "    def __init__(self, train_iter):\r\n",
    "        self.labels, self.text = [], []\r\n",
    "        for label, line in train_iter:\r\n",
    "            self.labels.append(label)\r\n",
    "            self.text.append(line)\r\n",
    "    \r\n",
    "    def __getitem__(self, index):\r\n",
    "        return self.labels[index], self.text[index]\r\n",
    "    \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.labels)\r\n",
    "\r\n",
    "train_dataset = dataset_from_iter(train_iter)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "batch_size = 64\r\n",
    "\r\n",
    "n_train = len(train_dataset)\r\n",
    "split = n_train // 3\r\n",
    "indices = list(range(n_train))\r\n",
    "random.shuffle(indices)\r\n",
    "train_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[split:])\r\n",
    "valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[:split])\r\n",
    "\r\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, collate_fn=collate_batch)\r\n",
    "valid_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=valid_sampler, collate_fn=collate_batch)\r\n",
    "test_dataloader = DataLoader(test_iter, batch_size=batch_size, collate_fn=collate_batch)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "class LSTM(nn.Module):\r\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout_rate, pad_index):\r\n",
    "        super().__init__()\r\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\r\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, bidirectional=bidirectional, dropout=dropout_rate)\r\n",
    "        self.fc = nn.Linear(hidden_dim*2 if bidirectional else hidden_dim, output_dim)\r\n",
    "        self.dropout = nn.Dropout(dropout_rate)\r\n",
    "    \r\n",
    "    def forward(self, text):\r\n",
    "        embedded = self.dropout(self.embedding(text))\r\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths=[embedded.size(1) for i in range(embedded.size(0))])\r\n",
    "\r\n",
    "        _, (hidden, _) = self.rnn(packed_embedded)\r\n",
    "\r\n",
    "        if self.rnn.bidirectional:\r\n",
    "            hidden = self.dropout(torch.cat([hidden[-1], hidden[-2]], dim=-1))\r\n",
    "            # [batch_size, hidden_dim * 2]\r\n",
    "        else:\r\n",
    "            hidden = self.dropout(hidden[-1])\r\n",
    "            # [batch_size, hidden_dim]\r\n",
    "        \r\n",
    "        predictions = self.fc(hidden)\r\n",
    "\r\n",
    "        return predictions\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "vocab_size = len(Vocab)\r\n",
    "embedding_dim = 300\r\n",
    "hidden_dim = 300\r\n",
    "output_dim = 1\r\n",
    "n_layers = 2\r\n",
    "bidirectional = True\r\n",
    "dropout_rate = 0.5\r\n",
    "\r\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout_rate, pad_index=Vocab.get_stoi()['<pad>'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "# TODO\r\n",
    "# add pretrained model\r\n",
    "\r\n",
    "lr = 5e-4\r\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\r\n",
    "criterion = nn.CrossEntropyLoss()\r\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "\r\n",
    "model = model.to(device)\r\n",
    "criterion = criterion.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "def binary_accuracy(preds, y):\r\n",
    "    rounded_preds = torch.round(preds)\r\n",
    "    correct = (rounded_preds == y).float()\r\n",
    "    acc = correct.sum() / len(correct)\r\n",
    "\r\n",
    "    return acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "def train(data_loader, model, criterion, optimizer, device):\r\n",
    "    model.train()\r\n",
    "    epoch_losses = []\r\n",
    "    epoch_accs = []\r\n",
    "\r\n",
    "    for labels, text in tqdm.tqdm(data_loader, desc='training...', file=sys.stdout):\r\n",
    "        labels = labels.to(device)\r\n",
    "        text = text.to(device)\r\n",
    "\r\n",
    "        predictions = model(text)\r\n",
    "        loss = criterion(predictions, labels)\r\n",
    "        acc = binary_accuracy(predictions, labels)\r\n",
    "\r\n",
    "        epoch_losses.append(loss.item)\r\n",
    "        epoch_accs.append(acc.item)\r\n",
    "\r\n",
    "        optimizer.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "\r\n",
    "    return epoch_losses, epoch_accs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "def evaluate(data_loader, model, criterion, device):\r\n",
    "    model.eval()\r\n",
    "    losses = []\r\n",
    "    accs = []\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        for labels, text in tqdm.tqdm(data_loader, desc='evaluating...', file=sys.stdout):\r\n",
    "            labels = labels.to(device)\r\n",
    "            text = text.to(device)\r\n",
    "\r\n",
    "            predictions = model(text)\r\n",
    "            loss = criterion(predictions, labels)\r\n",
    "            acc = binary_accuracy(predictions, labels)\r\n",
    "\r\n",
    "            losses.append(loss)\r\n",
    "            accs.append(acc)\r\n",
    "        \r\n",
    "        return losses, accs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_epoch = 10\r\n",
    "best_valid_loss = float('inf')\r\n",
    "train_losses = []\r\n",
    "train_accs = []\r\n",
    "valid_losses = []\r\n",
    "valid_accs = []\r\n",
    "\r\n",
    "for epoch in range(n_epoch):\r\n",
    "    train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, device)\r\n",
    "    valid_loss, valid_acc = evaluate(valid_dataloader, model, criterion, device)\r\n",
    "\r\n",
    "    train_losses.extend(train_loss)\r\n",
    "    train_accs.extend(train_acc)\r\n",
    "    valid_losses.extend(valid_loss)\r\n",
    "    valid_accs.extend(valid_acc)\r\n",
    "\r\n",
    "    train_loss_mean = np.mean(train_loss)\r\n",
    "    train_acc_mean = np.mean(train_acc)\r\n",
    "    valid_loss_mean = np.mean(valid_loss)\r\n",
    "    valid_acc_mean = np.mean(valid_acc)\r\n",
    "\r\n",
    "    if valid_loss_mean < best_valid_loss:\r\n",
    "        best_valid_loss = valid_loss_mean\r\n",
    "        torch.save(model.state_dict(), 'best_model')\r\n",
    "    \r\n",
    "    print(f'epoch: {epoch+1:02}')\r\n",
    "    print(f'train loss: {train_loss_mean:.3f}, train acc: {train_acc_mean:.3f}')\r\n",
    "    print(f'valid loss: {train_loss_mean:.3f}, valid acc: {train_acc_mean:.3f}')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('nlp': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "interpreter": {
   "hash": "ef35f4d57919dd7858b7d36ee9ae1e0dc52bc378b50574145c8845a25779d1f8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}