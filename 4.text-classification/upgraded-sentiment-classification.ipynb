{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import functools\r\n",
    "import sys\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torchtext\r\n",
    "import tqdm\r\n",
    "\r\n",
    "print(sys.executable)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "C:\\Users\\Tyrone\\anaconda3\\envs\\nlp\\python.exe\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_iter = torchtext.datasets.IMDB(split='train')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "tokenizor = torchtext.data.utils.get_tokenizer('basic_english')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "tokens = []\r\n",
    "for label, line in train_iter:\r\n",
    "    tokens += tokenizor(line)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from collections import Counter, OrderedDict\r\n",
    "# from torchtext.vocab import Vocab\r\n",
    "\r\n",
    "counter = Counter(tokens) # unsorted dict\r\n",
    "sorted_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\r\n",
    "ordered_dict = OrderedDict(sorted_tuples)\r\n",
    "Vocab = torchtext.vocab.vocab(ordered_dict, min_freq=10)\r\n",
    "\r\n",
    "# adding <unk> token and default index\r\n",
    "unk_token = '<unk>'\r\n",
    "default_index = 0\r\n",
    "if unk_token not in Vocab:\r\n",
    "    Vocab.insert_token(unk_token, default_index)\r\n",
    "Vocab.set_default_index(default_index)\r\n",
    "\r\n",
    "# adding <pad> token\r\n",
    "Vocab.insert_token('<pad>', 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "from torch.utils.data import DataLoader, Dataset\r\n",
    "from torch.nn.utils.rnn import pad_sequence\r\n",
    "import random\r\n",
    "\r\n",
    "text_transform = lambda x: [Vocab[token] for token in tokenizor(x)]\r\n",
    "label_transform = lambda x: 1 if x == 'pos' else 0\r\n",
    "\r\n",
    "def collate_batch(batch):\r\n",
    "    label_list, text_list = [], []\r\n",
    "    for label, text in batch:\r\n",
    "        text_list.append(torch.tensor(text_transform(text)))\r\n",
    "        label_list.append(label_transform(label))\r\n",
    "    \r\n",
    "    return torch.tensor(label_list), pad_sequence(text_list, padding_value=Vocab['<pad>'])\r\n",
    "\r\n",
    "train_iter, test_iter = torchtext.datasets.IMDB(split=('train', 'test'))\r\n",
    "\r\n",
    "class dataset_from_iter(Dataset):\r\n",
    "    def __init__(self, train_iter):\r\n",
    "        self.labels, self.text = [], []\r\n",
    "        for label, line in train_iter:\r\n",
    "            self.labels.append(label)\r\n",
    "            self.text.append(line)\r\n",
    "    \r\n",
    "    def __getitem__(self, index):\r\n",
    "        return self.labels[index], self.text[index]\r\n",
    "    \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.labels)\r\n",
    "\r\n",
    "train_dataset = dataset_from_iter(train_iter)\r\n",
    "\r\n",
    "n_train = len(train_dataset)\r\n",
    "split = n_train // 3\r\n",
    "indices = list(range(n_train))\r\n",
    "random.shuffle(indices)\r\n",
    "train_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[split:])\r\n",
    "valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[:split])\r\n",
    "\r\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, sampler=train_sampler, collate_fn=collate_batch)\r\n",
    "valid_dataloader = DataLoader(train_dataset, batch_size=8, sampler=valid_sampler, collate_fn=collate_batch)\r\n",
    "test_dataloader = DataLoader(test_iter, batch_size=8, collate_fn=collate_batch)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "class LSTM(nn.Module):\r\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout_rate, pad_index):\r\n",
    "        super().__init__()\r\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\r\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, bidirectional=bidirectional, dropout=dropout_rate)\r\n",
    "        self.fc = nn.Linear(hidden_dim*2 if bidirectional else hidden_dim, output_dim)\r\n",
    "        self.dropout = nn.Dropout(dropout_rate)\r\n",
    "    \r\n",
    "    def forward(self, text):\r\n",
    "        embedded = self.dropout(self.embedding(text))\r\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths=[embedded.size(1) for i in range(embedded.size(0))])\r\n",
    "\r\n",
    "        _, (hidden, _) = self.rnn(packed_embedded)\r\n",
    "\r\n",
    "        if self.rnn.bidirectional:\r\n",
    "            hidden = self.dropout(torch.cat([hidden[-1], hidden[-2]], dim=-1))\r\n",
    "            # [batch_size, hidden_dim * 2]\r\n",
    "        else:\r\n",
    "            hidden = self.dropout(hidden[-1])\r\n",
    "            # [batch_size, hidden_dim]\r\n",
    "        \r\n",
    "        predictions = self.fc(hidden)\r\n",
    "\r\n",
    "        return predictions\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "vocab_size = len(Vocab)\r\n",
    "embedding_dim = 300\r\n",
    "hidden_dim = 300\r\n",
    "output_dim = 1\r\n",
    "n_layers = 2\r\n",
    "bidirectional = True\r\n",
    "dropout_rate = 0.5\r\n",
    "\r\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout_rate, pad_index=Vocab.get_stoi()['<pad>'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# TODO\r\n",
    "# add pretrained model\r\n",
    "\r\n",
    "lr = 5e-4\r\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\r\n",
    "criterion = nn.CrossEntropyLoss()\r\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "\r\n",
    "model = model.to(device)\r\n",
    "criterion = criterion.to(device)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(20437, 300, padding_idx=1)\n",
       "  (rnn): LSTM(300, 300, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "  (fc): Linear(in_features=600, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('nlp': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "interpreter": {
   "hash": "ef35f4d57919dd7858b7d36ee9ae1e0dc52bc378b50574145c8845a25779d1f8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}