{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\r\n",
    "import torchvision\r\n",
    "import numpy as np\r\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "SEED = 1234\r\n",
    "\r\n",
    "random.seed(SEED)\r\n",
    "np.random.seed(SEED)\r\n",
    "torch.manual_seed(SEED)\r\n",
    "torch.cuda.manual_seed(SEED)\r\n",
    "torch.backends.cudnn.deterministic = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ROOT = '.data'\r\n",
    "\r\n",
    "train_data = torchvision.datasets.MNIST(root=ROOT, train=True, download=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mean = train_data.data.float().mean() / 225\r\n",
    "std = train_data.data.float().std() / 225\r\n",
    "\r\n",
    "print(f'data mean: {mean}')\r\n",
    "print(f'data std: {std}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torchvision import transforms\r\n",
    "\r\n",
    "train_transforms = transforms.Compose([\r\n",
    "    transforms.RandomRotation(5, fill=(0, )),\r\n",
    "    transforms.RandomCrop(28, padding=2),\r\n",
    "    transforms.ToTensor(),\r\n",
    "    transforms.Normalize(mean=[mean], std=[std])\r\n",
    "])\r\n",
    "\r\n",
    "test_transforms = transforms.Compose([\r\n",
    "    transforms.ToTensor(),\r\n",
    "    transforms.Normalize(mean=[mean], std=[std])\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data = torchvision.datasets.MNIST(root=ROOT,\r\n",
    "                                        train=True,\r\n",
    "                                        download=True,\r\n",
    "                                        transform=train_transforms)\r\n",
    "\r\n",
    "test_data = torchvision.datasets.MNIST(root=ROOT,\r\n",
    "                                       train=False,\r\n",
    "                                       download=True,\r\n",
    "                                       transform=test_transforms)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\r\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "def plot_images(images):\r\n",
    "    n_images = len(images)\r\n",
    "    rows = cols = int(np.sqrt(n_images))\r\n",
    "\r\n",
    "    fig = plt.figure()\r\n",
    "    for i in range(rows * cols):\r\n",
    "        ax = fig.add_subplot(rows, cols, i+1)\r\n",
    "        ax.imshow(images[i].view(28, 28).cpu().numpy(), cmap='bone')\r\n",
    "        ax.axis('off')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "N_IMAGES = 25\r\n",
    "\r\n",
    "# images = [image for image, label in [train_data[i] for i in range(N_IMAGES)]]\r\n",
    "images = [train_data[i][0] for i in range(N_IMAGES)]\r\n",
    "\r\n",
    "plot_images(images)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "VALID_RATIO = 0.1\r\n",
    "\r\n",
    "n_train_examples = int(len(train_data) * (1 - VALID_RATIO))\r\n",
    "n_test_examples = int(len(train_data) * VALID_RATIO)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data, valid_data = torch.utils.data.random_split(train_data, [n_train_examples, n_test_examples])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\r\n",
    "print(f'Number of validation examples: {len(valid_data)}')\r\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import copy\r\n",
    "\r\n",
    "valid_data = copy.deepcopy(valid_data)\r\n",
    "valid_data.dataset.transform = test_transforms"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch.utils.data import DataLoader\r\n",
    "\r\n",
    "BATCH_SIZE = 256\r\n",
    "\r\n",
    "train_iterator = DataLoader(train_data, BATCH_SIZE, True)\r\n",
    "valid_iterator = DataLoader(valid_data, BATCH_SIZE, False)\r\n",
    "test_iterator = DataLoader(test_data, BATCH_SIZE, False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch import nn\r\n",
    "\r\n",
    "class MLP(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        self.fc_1 = nn.Linear(784, 250)\r\n",
    "        self.fc_2 = nn.Linear(250, 100)\r\n",
    "        self.fc_3 = nn.Linear(100, 10)\r\n",
    "        self.relu = nn.ReLU()\r\n",
    "    \r\n",
    "    def forward(self, batch):\r\n",
    "        batch_size = batch.shape[0]\r\n",
    "\r\n",
    "        batch = batch.view(batch_size, -1)\r\n",
    "\r\n",
    "        h_1 = self.relu(self.fc_1(batch))\r\n",
    "        h_2 = self.relu(self.fc_2(h_1))\r\n",
    "        \r\n",
    "        preds = self.fc_3(h_2)\r\n",
    "\r\n",
    "        return preds, h_2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = MLP()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def count_parameters(model):\r\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
    "\r\n",
    "count_parameters(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\r\n",
    "\r\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "\r\n",
    "model.to(device)\r\n",
    "criterion.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def calculate_accuracy(preds, labels):\r\n",
    "    top_preds = torch.argmax(preds, dim=1, keepdim=True)\r\n",
    "    correct = top_preds.eq(labels.view_as(top_preds)).sum()\r\n",
    "    acc = correct.float() / labels.shape[0]\r\n",
    "\r\n",
    "    return acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\r\n",
    "    epoch_loss = 0\r\n",
    "    epoch_acc = 0\r\n",
    "\r\n",
    "    model.train()\r\n",
    "\r\n",
    "    for images, labels in iterator:\r\n",
    "        images, labels = images.to(device), labels.to(device)\r\n",
    "\r\n",
    "        preds, _ = model(images)\r\n",
    "\r\n",
    "        loss = criterion(preds, labels)\r\n",
    "        acc = calculate_accuracy(preds, labels)\r\n",
    "\r\n",
    "        optimizer.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "\r\n",
    "        epoch_loss += loss.item()\r\n",
    "        epoch_acc += acc.item()\r\n",
    "    \r\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluate(model, iterator, criterion, device):\r\n",
    "    epoch_loss = 0\r\n",
    "    epoch_acc = 0\r\n",
    "\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        for images, labels in iterator:\r\n",
    "            images, labels = images.to(device), labels.to(device)\r\n",
    "\r\n",
    "            preds, _ = model(images)\r\n",
    "\r\n",
    "            loss = criterion(preds, labels)\r\n",
    "            acc = calculate_accuracy(preds, labels)\r\n",
    "\r\n",
    "            epoch_loss += loss.item()\r\n",
    "            epoch_acc += acc.item()\r\n",
    "    \r\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "N_EPOCH = 10\r\n",
    "\r\n",
    "best_valid_loss = float('inf')\r\n",
    "\r\n",
    "for epoch in range(N_EPOCH):\r\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\r\n",
    "    valid_loss, valid_acc = evaluate(model, train_iterator, criterion, device)\r\n",
    "\r\n",
    "    if valid_loss < best_valid_loss:\r\n",
    "        best_valid_loss = valid_loss\r\n",
    "        torch.save(model.state_dict(), 'mlp-model')\r\n",
    "    \r\n",
    "    print(f'Epoch: {epoch+1:02}')\r\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\r\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.load_state_dict(torch.load('mlp-model'))\r\n",
    "\r\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, device)\r\n",
    "\r\n",
    "print(f'\\t Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch.nn.functional as F\r\n",
    "\r\n",
    "def get_prediction(model, iterator, device):\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    images, labels, preds = [], [], []\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        for image, label in iterator:\r\n",
    "            image = image.to(device)\r\n",
    "            \r\n",
    "            pred, _ = model(image)\r\n",
    "            pred = F.softmax(pred, dim=-1)\r\n",
    "            # pred = torch.argmax(pred, dim=-1)\r\n",
    "\r\n",
    "            images.append(image.cpu())\r\n",
    "            labels.append(label)\r\n",
    "            preds.append(pred.cpu())\r\n",
    "    \r\n",
    "    images = torch.cat(images, dim=0)\r\n",
    "    labels = torch.cat(labels, dim=0)\r\n",
    "    preds = torch.cat(preds, dim=0)\r\n",
    "\r\n",
    "    return images, labels, preds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn import metrics\r\n",
    "\r\n",
    "def plot_confusion_matrix(labels, preds):\r\n",
    "    fig = plt.figure(figsize = (10, 10))\r\n",
    "    ax = fig.add_subplot(1, 1, 1)\r\n",
    "    cm = metrics.confusion_matrix(labels, preds)\r\n",
    "    cm = metrics.ConfusionMatrixDisplay(cm, display_labels = range(10))\r\n",
    "    cm.plot(values_format = 'd', cmap = 'Blues', ax = ax)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "images, labels, preds = get_prediction(model, test_iterator, device)\r\n",
    "\r\n",
    "plot_confusion_matrix(labels, torch.argmax(preds, dim=1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_most_incorrect(incorrect_examples, n_images):\r\n",
    "\r\n",
    "    rows = int(np.sqrt(n_images))\r\n",
    "    cols = int(np.sqrt(n_images))\r\n",
    "\r\n",
    "    fig = plt.figure(figsize = (20, 10))\r\n",
    "    for i in range(rows*cols):\r\n",
    "        ax = fig.add_subplot(rows, cols, i+1)\r\n",
    "        image, true_label, probs = incorrect_examples[i]\r\n",
    "        true_prob = probs[true_label]\r\n",
    "        incorrect_prob, incorrect_label = torch.max(probs, dim=0)\r\n",
    "        ax.imshow(image.view(28, 28).cpu().numpy(), cmap='bone')\r\n",
    "        ax.set_title(f'true label: {true_label} ({true_prob:.3f})\\n' \\\r\n",
    "                     f'pred label: {incorrect_label} ({incorrect_prob:.3f})')\r\n",
    "        ax.axis('off')\r\n",
    "    fig.subplots_adjust(hspace= 0.5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corrects = torch.eq(labels, torch.argmax(preds, dim=1))\r\n",
    "\r\n",
    "incorrect_examples = []\r\n",
    "for image, label, pred, correct in zip(images, labels, preds, corrects):\r\n",
    "    if not correct:\r\n",
    "        incorrect_examples.append((image, label, pred))\r\n",
    "\r\n",
    "incorrect_examples.sort(reverse=True, key=lambda x: torch.max(x[2], dim=0).values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "N_IMAGES = 25\r\n",
    "\r\n",
    "plot_most_incorrect(incorrect_examples, N_IMAGES)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('pytorch-course': conda)"
  },
  "interpreter": {
   "hash": "f5d9901711b75ab1d5ebc89a95a480a3d560bf8866d1ce73f5d45067d6c464d2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}